{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    num_cuda_devices = torch.cuda.device_count()\n",
    "    print(\"Number of CUDA devices:\", num_cuda_devices)\n",
    "    if num_cuda_devices > 0:\n",
    "        print(\"CUDA device name:\", torch.cuda.get_device_name(0))\n",
    "        print(\"CUDA device capability:\", torch.cuda.get_device_capability(0))\n",
    "    else:\n",
    "        print(\"No CUDA devices found despite CUDA being available\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    train_freq=4,\n",
    "    gradient_steps=-1,\n",
    "    gamma=0.99,\n",
    "    exploration_fraction=0.12,\n",
    "    exploration_final_eps=0.1,\n",
    "    target_update_interval=250,\n",
    "    learning_starts=0,\n",
    "    buffer_size=50000,\n",
    "    batch_size=128,\n",
    "    learning_rate=6.3e-4,\n",
    "    policy_kwargs=dict(net_arch=[256, 256]),\n",
    "    # tensorboard_log=tensorboard_log,\n",
    "    seed=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(\n",
    "    dqn_model,\n",
    "    dqn_model.get_env(),\n",
    "    deterministic=True,\n",
    "    n_eval_episodes=20,\n",
    ")\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_model.learn(int(1e5), log_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(dqn_model, dqn_model.get_env(), deterministic=True, n_eval_episodes=20)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Q Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_values(model: DQN, obs: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Retrieve Q-values for a given observation.\n",
    "\n",
    "    :param model: a DQN model\n",
    "    :param obs: a single observation\n",
    "    :return: the associated q-values for the given observation\n",
    "    \"\"\"\n",
    "    assert model.get_env().observation_space.contains(obs), f\"Invalid observation of shape {obs.shape}: {obs}\"\n",
    "    obs_tensor = th.tensor(obs).float().unsqueeze(0).to(model.device)\n",
    "    with th.no_grad():\n",
    "        q_values = model.q_net.forward(obs_tensor).cpu().numpy().squeeze()\n",
    "\n",
    "    assert isinstance(q_values, np.ndarray), \"The returned q_values is not a numpy array\"\n",
    "    assert q_values.shape == (4,), f\"Wrong shape: (4,) was expected but got {q_values.shape}\"\n",
    "\n",
    "    return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "print(\"obs.shape:\", obs.shape)\n",
    "\n",
    "print (\"env.action_space:\", env.action_space)\n",
    "print(\"observation_space_shape:\",env.observation_space.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis('off')\n",
    "plt.imshow(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_str = [\"Nothing\", \"Left Engine\", \"Main Engine\", \"Right Engine\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state,_=env.reset()\n",
    "print(env.step,\"\\n\")\n",
    "print(env.action_space,\"\\n\")\n",
    "print(env.metadata,\"\\n\")\n",
    "print(env.observation_space,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_values = get_q_values(dqn_model, initial_state)\n",
    "print(q_values)\n",
    "q_value_nothing = q_values[0]\n",
    "q_value_left = q_values[1]\n",
    "q_value_main = q_values[2]\n",
    "q_value_right=q_values[3]\n",
    "\n",
    "print(f\"Q-value of the initial state left={q_value_left:.2f} nothing={q_value_nothing:.2f} right={q_value_right:.2f}\")\n",
    "\n",
    "action = np.argmax(q_values)\n",
    "\n",
    "print(f\"Action taken by the greedy policy in the initial state: {action_str[action]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_q_value = q_values.max()\n",
    "print(initial_q_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obs.shape)\n",
    "print(obs)\n",
    "obs = obs.flatten()\n",
    "print(obs.shape)\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "episode_rewards = []\n",
    "done = False\n",
    "i = 0\n",
    "\n",
    "while not done:\n",
    "    i += 1\n",
    "    \n",
    "    # Clear the previous figure\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Display current state\n",
    "    plt.imshow(env.render())\n",
    "    plt.show()\n",
    "\n",
    "    # Retrieve q-value\n",
    "    q_values = get_q_values(dqn_model, obs)\n",
    "\n",
    "    # Take greedy-action\n",
    "    action, _ = dqn_model.predict(obs, deterministic=True)\n",
    "\n",
    "    print(f\"Q-value of the current state \\nnothing={q_values[0]:.2f} \\nleft={q_values[1]:.2f} \\nmain={q_values[2]:.2f} \\nright={q_values[3]}\")\n",
    "    print(f\"Action: {action_str[action]}\")\n",
    "\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    done = terminated or truncated\n",
    "\n",
    "    episode_rewards.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_discounted_rewards = 0\n",
    "for i, reward in enumerate(reversed(episode_rewards)):\n",
    "    sum_discounted_rewards += reward * (dqn_model.gamma ** i)\n",
    "\n",
    "print(f\"Sum of discounted rewards: {sum_discounted_rewards:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "\n",
    "class DoubleDQN(DQN):\n",
    "    def train(self, gradient_steps: int, batch_size: int = 100) -> None:\n",
    "        self.policy.set_training_mode(True)\n",
    "        self._update_learning_rate(self.policy.optimizer)\n",
    "\n",
    "        losses = []\n",
    "        for _ in range(gradient_steps):\n",
    "            # Sample replay buffer\n",
    "            replay_data = self.replay_buffer.sample(batch_size, env=self._vec_normalize_env)\n",
    "\n",
    "            with th.no_grad():\n",
    "                # Compute the next Q-values using the target network\n",
    "                next_q_values = self.q_net_target(replay_data.next_observations)\n",
    "                # Compute q-values for the next observation using the online q net\n",
    "                next_q_values_online = self.q_net(replay_data.next_observations)\n",
    "                # Select action with online network\n",
    "                next_actions_online = next_q_values_online.argmax(dim=1)\n",
    "                # Estimate the q-values for the selected actions using target q network\n",
    "                next_q_values = next_q_values.gather(1, next_actions_online.unsqueeze(1)).squeeze(1)\n",
    "                # 1-step TD target\n",
    "                target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values\n",
    "\n",
    "            # Get current Q-values estimates\n",
    "            current_q_values = self.q_net(replay_data.observations)\n",
    "            # Retrieve the q-values for the actions from the replay buffer\n",
    "            current_q_values = th.gather(current_q_values, dim=1, index=replay_data.actions.long())\n",
    "\n",
    "            # Compute loss (Huber loss)\n",
    "            loss = F.smooth_l1_loss(current_q_values, target_q_values)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Optimize the policy\n",
    "            self.policy.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
    "            self.policy.optimizer.step()\n",
    "\n",
    "        self._n_updates += gradient_steps\n",
    "\n",
    "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
    "        self.logger.record(\"train/loss\", np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "\n",
    "class MonitorQValueCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback to monitor the evolution of the q-value\n",
    "    for the initial state.\n",
    "    It allows to artificially over-estimate a q-value for initial states.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, sample_interval: int = 2500):\n",
    "        super().__init__()\n",
    "        self.timesteps = []\n",
    "        self.max_q_values = []\n",
    "        self.sample_interval = sample_interval\n",
    "        n_samples = 512\n",
    "        env = gym.make(\"LunarLander-v2\")\n",
    "        # Sample initial states that will be used to monitor the estimated q-value\n",
    "        self.start_obs = np.array([env.reset()[0] for _ in range(n_samples)])\n",
    "    \n",
    "    def _on_training_start(self) -> None:\n",
    "        # Create overestimation\n",
    "        obs = th.tensor(self.start_obs, device=self.model.device).float()\n",
    "        # Over-estimate going left q-value for the initial states\n",
    "        target_q_values = th.ones((len(obs), 1), device=self.model.device).float() * 100\n",
    "\n",
    "        for _ in range(100):\n",
    "            # Get current Q-values estimates\n",
    "            current_q_values = self.model.q_net(obs)\n",
    "\n",
    "            # Over-estimate going left\n",
    "            current_q_values = th.gather(current_q_values, dim=1, index=th.zeros((len(obs), 1), device=self.model.device).long())\n",
    "\n",
    "            loss = F.mse_loss(current_q_values, target_q_values)\n",
    "\n",
    "            # Optimize the policy\n",
    "            self.model.policy.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.model.policy.optimizer.step()\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Sample q-values\n",
    "        if self.n_calls % self.sample_interval == 0:\n",
    "            # Monitor estimated q-values using current model\n",
    "            obs = th.tensor(self.start_obs, device=self.model.device).float()\n",
    "            with th.no_grad():\n",
    "                q_values = self.model.q_net(obs).cpu().numpy()\n",
    "\n",
    "            self.logger.record(\"train/max_q_value\", float(q_values.max()))\n",
    "            self.timesteps.append(self.num_timesteps)\n",
    "            self.max_q_values.append(q_values.max())\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_dqn_value_cb = MonitorQValueCallback()\n",
    "print(monitor_dqn_value_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_q = DoubleDQN(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    train_freq=4,\n",
    "    gradient_steps=-1,\n",
    "    gamma=0.99,\n",
    "    exploration_fraction=0.12,\n",
    "    exploration_final_eps=0.1,\n",
    "    target_update_interval=250,\n",
    "    learning_starts=0,\n",
    "    buffer_size=50000,\n",
    "    batch_size=128,\n",
    "    learning_rate=6.3e-4,\n",
    "    policy_kwargs=dict(net_arch=[256, 256]),\n",
    "    # tensorboard_log=tensorboard_log,\n",
    "    seed=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_double_q_value_cb = MonitorQValueCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_dqn_value_cb = MonitorQValueCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_model.learn(int(1e5), log_interval=10, callback=monitor_dqn_value_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_q.learn(int(1e5), log_interval=10, callback=monitor_double_q_value_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3), dpi=150)\n",
    "plt.title(\"Evolution of max q-value for start states over time\")\n",
    "plt.plot(monitor_dqn_value_cb.timesteps, monitor_dqn_value_cb.max_q_values, label=\"DQN\", color=\"pink\")\n",
    "plt.plot(monitor_double_q_value_cb.timesteps, monitor_double_q_value_cb.max_q_values, label=\"Double DQN\", color=\"purple\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
